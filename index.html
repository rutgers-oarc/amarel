<!DOCTYPE html>
<html lang=en>

<head>
    <meta charset=utf-8>
    <meta content="IE=edge" http-equiv=X-UA-Compatible>
    <meta content="width=device-width,initial-scale=1" name=viewport>
    <meta content="Amarel User Guide" name=description>
    <meta content="Galen Collier" name=author>
    <title> Amarel User Guide &middot; OARC </title>
    <link href=bootstrap.unminified.css rel=stylesheet>
    <link href=main.unminified.css rel=stylesheet>
    <link href=favicon.ico rel=icon>
</head>

<body>
    <a href=#content class="sr-only sr-only-focusable" id=skippy>
    
    <header class="bs-docs-nav navbar navbar-static-top" id=top>
        <div class=container>
            <div class=navbar-header>
                <button aria-controls=bs-navbar aria-expanded=false class="collapsed navbar-toggle" data-target=#bs-navbar data-toggle=collapse type=button> <span class=sr-only>Toggle navigation</span> <span class=icon-bar></span> <span class=icon-bar></span> <span class=icon-bar></span> </button> <a href=http://oarc.rutgers.edu/ class=navbar-brand>OARC Home</a> </div>
            <nav class="collapse navbar-collapse" id=bs-navbar>
                <ul class="nav navbar-nav">
                    <li class=active> <a href=../amarel/index.html>Amarel User Guide</a>
                    </li>
                    <li> <a href=https://rutgers-oarc.github.io/perceval>Perceval</a>
                    </li>
                    <li> <a href=../experimental/>Experimental Platforms</a>
                    </li>
                    <li> <a href=https://rutgers-oarc.github.io/community>Community Resources</a>
                    </li>
                </ul>
                <ul class="nav navbar-nav navbar-right">
                    <li><a href=http://oarc.rutgers.edu/events>Events</a></li>
                    <li><a href=http://oarc.rutgers.edu onclick='ga("send","event","Navbar","Community links","Research")'>Research</a></li>
                    <li><a href=https://oarc.rutgers.edu/people onclick='ga("send","event","Navbar","Community links","People")'>People</a></li>
                </ul>
            </nav>
        </div>
    </header>
    <div class=bs-docs-header id=content tabindex=-1>
        <div class=container>
            <h1>Amarel User Guide</h1>
            <p>A comprehensive guide to using OARC's community-focused condominium-style research computing cluster: instructions, best practices, and useful tips</p>
        </div>
    </div>
    <div class="container bs-docs-container">
        <div class=row>
            <div class=col-md-9 role=main>

                <div class=bs-docs-section>
                <h1 class=page-header id=status>Need Help?</h1>
                <p class=lead>Send us a Message</p>

                <p>Please contact our support team at <a href="mailto:help@hpc.rutgers.edu">help@hpc.rutgers.edu</a> and be sure to include "Amarel" somewhere in the subject line. Messages sent to this address will be seen by all members of our support team and whomever can best address your request will respond. Support is typically available 9am-5pm ET Mon-Fri.</p><br>

                <p class=lead>An important note about Email</p>
                <p>You must use an official Rutgers email account to communicate with the Amarel support team. The University's <a href=http://policies.rutgers.edu/7011-currentpdf-0>Acceptable Use Policy</a> states that all University business be conducted using the Official University email and calendar service, Rutgers Connect. This is necessary in order to meet federal, state and local, legal, regulatory and statutory requirements (e.g., HIPAA, OPRA, FERPA, GLBA); and in a manner which ensures business continuity, enables proper E-Discovery, and handles data in a secure and compliant manner.</p><br>

                <p class=lead>Office Hours</p>
                <p>We have regular open office hours, no appointment needed, nearly every week. This term (Spring 2018), our office hours session is usually held on Wednesdays between 12-3pm ET in Piscataway (CoRE 708) and in Newark (MSB C-630).</p>
                <p>Of course, we're happy to meet with you in our offices or at a location convenient to you at other times. Send us a message and we'll schedule a meeting</p>
                </div>

                <div class=bs-docs-section>
                    <h1 class=page-header id=status>Current Status</h1>
                    <p class=lead>All systems are in "normal" production mode. No system-wide maintenance scheduled.</p>
                </div>

                <div class=bs-docs-section>
                    <h1 class=page-header id=generalinfo>General information</h1>
                    <p class=lead>Amarel is a CentOS 7 Linux compute cluster that is actively growing through the combination of separate computing clusters into a single, shared resource.</p>
                    <div>
                    <p>Amarel includes the following hardware (this list may already be outdated since the cluster is actively growing):
                    </p>
                    <p>
                    52 CPU-only nodes, each with 28 Xeon e5-2680v4 (Broadwell) cores + 128 GB RAM<br>
                    20 CPU-only nodes, each with 28 Xeon e5-2680v4 (Broadwell) cores + 256 GB RAM<br>
                    4 28-core e5-2680v4 nodes each with 2 x Nvidia Pascal P100 GPUs onboard<br>
                    2 high-memory nodes, each with 56 e7-4830v4 (Broadwell) cores + 1.5 TB RAM<br>
                    53 CPU-only nodes, each with 16 Intel Xeon e5-2670 (Sandy Bridge) cores + 128 GB RAM<br>
                    5 CPU-only nodes, each with 20 Intel Xeon e5-2670 (Ivy Bridge) cores + 128 GB RAM<br>
                    26 CPU-only nodes, each with 24 Intel Xeon e5-2670 (Haswell) cores + 128 GB RAM<br>
                    4 CPU-only nodes, each with 16 Intel Xeon e5-2680 (Broadwell) cores + 128 GB RAM<br>
                    3 12-core e5-2670 nodes with 8 Nvidia Tesla M2070 GPUs onboard<br>
                    2 28-core e5-2680 nodes with 4 Quadro M6000 GPUs onboard<br>
                    1 16-core e5-2670 node with 8 Xeon Phi 5110P accelerators onboard<br>
                    <br>
                    Default run time = 2 hours in the 'main' partition<br>
                    Maximum run time = 3 days in the 'main' partition</p>
                    </div>
                </div>


                <div class=bs-docs-section>
                    <h1 class=page-header id=connecting>Connecting to Amarel</h1>
                    <p>Amarel is currently accessed using a single hostname, amarel.rutgers.edu</p>
                    <p>When you connect to this system, your log-in session (your Linux shell) will begin on one of multiple log-in nodes, named amarel1, amarel2, etc.</p>
                    <p>So, while you are logged-in to Amarel, you will see "amarel1" or "amarel2" as the name of the machine you are using.</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>ssh [your NetID]@amarel.rutgers.edu</code></pre></figure>
                    <p>If you are connecting from a location outside the Rutgers campus network, you will need to first connect to the campus network using the Rutgers VPN (virtual private network) service. See <a href=https://oit.rutgers.edu/vpn>https://oit.rutgers.edu/vpn</a> for details.
                </div>



                <div class=bs-docs-section>
                    <h1 class=page-header id=movingfiles>Moving files</h1>
                    <p>There are many different ways to this: secure copy (scp), remote sync (rsync), an FTP client (FileZilla), etc. Let’s assume you’re logged-in to a local workstation or laptop (not already logged-in to Amarel). To send files from your local system to your Amarel /home directory,</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>scp file-1.txt file-2.txt [NetID]@amarel.rutgers.edu:/home/[NetID]</code></pre></figure>
                    <p>To pull a file from your Amarel /home directory to your laptop (note the “.” at the end of this command),</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>scp [NetID]@amarel.rutgers.edu:/home/[NetID]/file-1.txt  . </code></pre></figure>
                    <p>If you want to copy an entire directory and its contents using scp, you’ll need to “package” your directory into a single, compressed file before moving it:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>tar -czf my-directory.tar.gz my-directory</code></pre></figure>
                    <p>After moving it, you can unpack that .tar.gz file to get your original directory and contents:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>tar -xzf my-directory.tar.gz</code></pre></figure>
                    <p>A handy way to synchronize a local file or entire directory between your local workstation and the Amarel cluster is to use the rsync utility. First, let's sync a local (recently updated) directory with the same directory stored on Amarel:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>rsync -trlvpz work-dir gc563@amarel.rutgers.edu:/home/gc563/work-dir</code></pre></figure>
                    <p>In this example, the rsync options I'm using are t (preserve modification times), r (recursive, sync all subdirectories), l (preserve symbolic links), v (verbose, show all details), p (preserve permissions), z (compress transferred data)</p>
                    <p>To sync a local directory with updated data from Amarel:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>rsync -trlvpz gc563@amarel.rutgers.edu:/home/gc563/work-dir work-dir</code></pre></figure>
                    <p>Here, we've simply reversed the order of the local and remote locations.</p>
                    <p>For added security, you can use SSH for the data transfer by adding the e option followed by the protocol name (ssh, in this case):</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>rsync -trlvpze ssh gc563@amarel.rutgers.edu:/home/gc563/work-dir work-dir</code></pre></figure>
                </div>


                <div class=bs-docs-section>
                    <h1 class=page-header id=listingresources>Listing available resources</h1>
                    <p>Before requesting resources (compute nodes), it’s helpful to see what resources are available and what cluster partitions (job queues) to use for certain resources.</p>
                    <p>Example of using the <strong>sinfo</strong> command:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>$ sinfo

PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
main*        up 3-00:00:00      4 drain* hal[0050-0051,0055,0093]
main*        up 3-00:00:00      5  down* slepner[084-088]
main*        up 3-00:00:00      4  drain hal[0023,0025-0027]
main*        up 3-00:00:00     86    mix gpu[003-004,006],hal[0001-0008,0017-0018,0022,0024,0028-0032,0044-0047,0054,0056-0057,0062-0068,0073-0079,0081-0092,0094-0096],mem002,pascal[001-006],slepner[010-014,016,018-023,036,042-044,046,048,071,074,076,081-082]
main*        up 3-00:00:00     84  alloc gpu005,hal[0009-0016,0019-0021,0033-0043,0048-0049,0052-0053,0058-0061,0069-0072,0080],mem001,slepner[009,015,017,024-035,037-041,045,047,054-070,072-073,075,077-080,083]
main*        up 3-00:00:00      2   down gpu[001-002]
gpu          up 3-00:00:00      8    mix gpu[003,006],pascal[001-006]
gpu          up 3-00:00:00      1  alloc gpu005
gpu          up 3-00:00:00      2   down gpu[001-002]
phi          up 3-00:00:00      1    mix gpu004
mem          up 3-00:00:00      1    mix mem002
mem          up 3-00:00:00      1  alloc mem001

                    </code></pre></figure>
                    <p>Understanding this output:</p>
                    <p>There are 4 basic partitions, main (traditional compute nodes, CPUs only), gpu (nodes with general-purpose GPU accelerators), mem (CPU-only nodes with 1.5 TB RAM), phi (CPU-only nodes with Xeon Phi coprocessors.</p>
                    <p>The upper limit for a job’s run time is 3 days (72 hours).</p>
                    <p>Allocated (alloc) nodes are currently running jobs.</p>
                    <p>Mixed (mix) nodes have jobs using some, but not all, CPU cores onboard.</p>
                    <p>Idle nodes are currently available for new jobs.</p>
                    <p>Drained (drain, drng) nodes are not available for use and may be offline for maintenance.</p>
                    <p>Slepner? Norse Mythology, "Sleipnir" 8-legged war horse (this made more sense when CPUs had 8 cores).</p>
                    <p>Hal? Hal is a dependable member of the Discovery One crew who does an excellent job of following instructions.</p>
                    <p>Pascal? French mathematician and the name of one of NVIDIA's GPU architectures.</p>
                    <p>CUDA? This is the name of a parallel computing platform and application programming interface (API) model created by Nvidia</p>
                </div>

                <div class=bs-docs-section>
                    <h1 class=page-header id=softwaremodules>Loading software modules</h1>
                    <p>When you first log-in, only basic system-wide tools are available automatically. To use a specific software package that is already installed, you can setup your environment using the module system.</p>
                    <p>The <strong>module avail</strong> command will show a list of the core (primary) modules available:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>$ module avail

-------------------------------------------------- /opt/sw/modulefiles/Core --------------------------------------------------
   ARACNE/20110228         blat/35                  gcc/4.9.3               intel_mkl/16.0.3 (D)    mvapich2/2.2        (D)
   HISAT2/2.0.4            bowtie2/2.2.6            gcc/4.9.4               intel_mkl/17.0.0        openmpi/2.1.1
   HISAT2/2.1.0     (D)    bowtie2/2.2.9     (D)    gcc/5.3                 intel_mkl/17.0.1        pgi/16.9
   MATLAB/R2017a           bwa/0.7.12               gcc/5.4          (D)    intel_mkl/17.0.2        pgi/16.10           (D)
   MATLAB/R2017b    (D)    bwa/0.7.13        (D)    hdf5/1.8.16             java/1.7.0_79           python/2.7.11
   Mathematica/11.1        cuda/7.5                 intel/16.0.1            java/1.8.0_66           python/2.7.12
   OpenCV/2.3.1            cuda/8.0                 intel/16.0.3     (D)    java/1.8.0_73           python/3.5.0
   STAR/2.5.2a             cuda/9.0          (D)    intel/16.0.4            java/1.8.0_121          python/3.5.2        (D)
   Trinotate/2.0.2         cudnn/7.0.3              intel/17.0.0            java/1.8.0_141          samtools/0.1.19
   bamtools/2.4.0          cufflinks/2.2.1          intel/17.0.1            java/1.8.0_152   (D)    samtools/1.2
   bcftools/1.2            delly/0.7.6              intel/17.0.2            modeller/9.16           samtools/1.3.1      (D)
   bedtools2/2.25.0        gaussian/03revE01        intel/17.0.4            moe/2016.0802           trinityrnaseq/2.1.1
   blast/2.6.0             gaussian/09revD01 (D)    intel_mkl/16.0.1        mvapich2/2.1</code></pre></figure>
                    <p>Understanding this output:</p>
                    <p>The packages with a (D) are the default versions for packages where multiple versions are available.</p>
                    <br>
                    <p>To see a comprehensive list of all available modules (not just the core modules) use the <strong>module spider</strong> command.</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>$ module spider

---------------------------------------------------------------------------------------------------------------
The following is a list of the modules currently available:
---------------------------------------------------------------------------------------------------------------
  ARACNE: ARACNE/20110228
    ARACNE: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context

  HISAT2: HISAT2/2.0.4, HISAT2/2.1.0
    HISAT2: graph-based alignment of next generation sequencing reads to a population of genomes

  HMMER: HMMER/3.1b2
    HMMER: biosequence analysis using profile hidden Markov models

  MATLAB: MATLAB/R2017a, MATLAB/R2017b
    MATLAB: The Language of Technical Computing

  Mathematica: Mathematica/11.1
    Wolfram Mathematica: Modern Technical Computing

  NAMD: NAMD/2.10
    NAMD: Scalable Molecular Dynamics

  ORCA: ORCA/3.0.3
    ORCA: An ab initio, DFT and semiempirical SCF-MO package

  OpenCV: OpenCV/2.3.1
    OpenCV: Open Source Computer Vision</code></pre></figure>
                <p>Loading a software module changes your environment settings so that the executable binaries, needed libraries, etc. are available for use.</p>
                <p>To load a software module, use the <strong>module load</strong> command, followed by the name and version desired.</p>
                <p>To remove select modules, use the <strong>module unload</strong> command. To remove all loaded software modules, use the <strong>module purge</strong> command.</p>
                <p>To load the default version of any software package, use the <strong>module load</strong> command but only specify the name of the package, not the version number.</p>
                <br>
                <p>Below are some examples.</p>
                <figure class=highlight><pre><code class=language-html data-lang=html>$ module load intel/16.0.3 mvapich2/2.1
$ module list
Currently Loaded Modules:
  1) intel/16.0.3   2) mvapich2/2.1

$ module unload mvapich2/2.1
$ module list
Currently Loaded Modules:
  1) intel/16.0.3

$ module purge
$ module list
No modules loaded

$ module load intel
$ module list
Currently Loaded Modules:
  1) intel/16.0.3</code></pre></figure>
                <p>If you always use the same software modules, your ~/.bashrc (a hidden login script located in your /home directory) can be configured to load those modules automatically every time you log in. Just add your desired <strong>module load</strong> command(s) to the end of that file. You can always edit your ~/.bashrc file to change or remove those commands later.</p>
                <p><strong>PLEASE NOTE:</strong> Software installed cluster-wide is typically configured with default or standard (basic) options, so special performance-enhancing features may not be enabled. This is because the Amarel cluster comprises a variety of hardware platforms and cluster-wide software installations must be compatible with all of the available hardware (including the older compute nodes). If the performance of the software you use for your research can be enhanced using hardware-specific options (targeting special CPU core instruction sets), you should consider installing your own customized version of that software in your /home directory.<p>
                </div>



                <div class=bs-docs-section>
                    <h1 class=page-header id=serialjobs>Running a serial (single-core) job</h1>
                    <p>Here’s an example of a SLURM job script for a serial job. I’m running a program called “zipper” which is in my /scratch (temporary work) directory. I plan to run my entire job from within my /scratch directory because that offers the best filesystem I/O performance.</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>#!/bin/bash

#SBATCH --partition=main             # Partition (job queue)
#SBATCH --requeue                    # Return job to the queue if preempted
#SBATCH --job-name=zipx001a          # Assign an short name to your job
#SBATCH --nodes=1                    # Number of nodes you require
#SBATCH --ntasks=1                   # Total # of tasks across all nodes
#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)
#SBATCH --mem=2000                   # Real memory (RAM) required (MB)
#SBATCH --time=02:00:00              # Total run time limit (HH:MM:SS)
#SBATCH --output=slurm.%N.%j.out     # STDOUT output file
#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional)
#SBATCH --export=ALL                 # Export you current env to the job env

cd /scratch/[your NetID]

module purge
module load intel/16.0.3 fftw/3.3.1

srun /scratch/[your NetID]/zipper/2.4.1/bin/zipper < my-input-file.in</code></pre></figure>
                    <p>Understanding this job script:</p>
                    <p>A job script contains the instructions for the SLURM workload manager (cluster job scheduler) to manage resource allocation, scheduling, and execution of your job.</p>
                    <p>The lines beginning with #SBATCH contain commands intended only for the workload manager.</p>
                    <p>My job will be assigned to the “main” partition (job queue).
                    <p>If this job is preempted, it will be returned to the job queue and will start again when required resources are available</p>
                    <p>This job will only use 1 CPU core and should not require much memory, so I have requested only 2 GB of RAM — it’s a good practice to request only about 2 GB per core for any job unless you know that your job will require more than that.</p>
                    <p>My job will be terminated when the run time limit has been reached, even if the program I’m running is not finished. It is not possible to extend this time after a job starts running.</p>
                    <p>Any output that would normally go to the command line will be redirected into the output file I have specified, and that file will be named using the compute node name and the job ID number.</p>
                    <p>Be sure to configure your environment as needed for running your application/executable. This usually means loading any needed modules before the step where you run your application/executable.</p>
                    <br>
                    <p>Here’s how to run a serial batch job, loading modules and using the <strong>sbatch</strong> command:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>$ sbatch my-job-script.sh</code></pre></figure>
                    <p>The <strong>sbatch</strong> command reads the contents of your job script and forwards those instructions to the SLURM workload manager. Depending on the level of activity on the cluster, your job may wait in the job queue for minutes or hours before it begins running.</p>
                </div>



                <div class=bs-docs-section>
                    <h1 class=page-header id=paralleljobs>Running a parallel (multicore MPI) job</h1>
                    <p>Here’s an example of a SLURM job script for a parallel job. See the previous (serial) example for some important details omitted here.</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>#!/bin/bash

#SBATCH --partition=main             # Partition (job queue)
#SBATCH --requeue                    # Return job to the queue if preempted
#SBATCH --job-name=zipx001a          # Assign an short name to your job
#SBATCH --nodes=1                    # Number of nodes you require
#SBATCH --ntasks=16                  # Total # of tasks across all nodes
#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)
#SBATCH --mem=124000                 # Real memory (RAM) required (MB)
#SBATCH --time=02:00:00              # Total run time limit (HH:MM:SS)
#SBATCH --output=slurm.%N.%j.out     # STDOUT output file
#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional)
#SBATCH --export=ALL                 # Export you current env to the job env

cd /scratch/[your NetID]

module purge
module load intel/16.0.3 fftw/3.3.1 mvapich2/2.1

srun --mpi=pmi2 /scratch/[your NetID]/zipper/2.4.1/bin/zipper < my-input-file.in</code></pre></figure>
                    <p>Understanding this job script:</p>
                    <p>The srun command is used to coordinate communication among the parallel tasks of your job. You must specify how many tasks you will be using, and this number usually matches the –ntasks value in your job’s hardware allocation request.</p>
                    <p>This job will use 16 CPU cores and nearly 8 GB of RAM per core, so I have requested a total of 124 GB of RAM — it’s a good practice to request only about 2 GB per core for any job unless you know that your job will require more than that.</p>
                    <p>Note here that I’m also loading the module for the parallel communication libraries (MPI libraries) needed by my parallel executable.</p>
                    <br>
                    <p>Here’s how to run a parallel batch job, loading modules and using the <strong>sbatch</strong> command:</p>

                    <figure class=highlight><pre><code class=language-html data-lang=html>
$ sbatch my-job-script.sh</code></pre></figure>
                </div>



                <div class=bs-docs-section>
                    <h1 class=page-header id=interactivejobs>Running an interactive job</h1>
                    <p>An interactive job gives you an active connection to a compute node (or collection of compute nodes) where you will have a login shell and you can run commands directly on the command line. This can be useful for testing, short analysis tasks, computational steering, or for running GUI-based applications.</p>

                    <p>When submitting an interactive job, you can request resources (single or multiple cores, memory, GPU nodes, etc.) just like you would in a batch job:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>[NetID@amarel1 ~]$ srun --partition=main --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2000 --time=00:30:00 --export=ALL --pty bash -i

srun: job 1365471 queued and waiting for resources
srun: job 1365471 has been allocated resources

[NetID@slepner045 ~]$</code></pre></figure>
                    <p>Notice that, when the interactive job is ready, the command prompt changes from NetID@amarel1 to NetID@slepner045. This change shows that I’ve been automatically logged-in to slepner045 and I’m now ready to run commands there. To exit this shell and return to the shell running on the amarel1 login node, type the <strong>exit</strong> command.</p>
                </div>



                <div class=bs-docs-section>
                    <h1 class=page-header id=monitoringjobs>Monitoring the status of jobs</h1>
                    <p>The simplest way to quickly check on the status of active jobs is by using the <strong>squeue</strong> command:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>$ squeue -u [your NetID]

  JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)
1633383      main   zipper    xx345   R       1:15      1 slepner36</code></pre></figure>
                    <p>Here, the state of each job is typically listed as being either PD (pending), R (running), along with the amount of allocated time that has been used (DD-HH:MM:SS).</p>

                    <p>For summary accounting information (including jobs that have already completed), you can use the <strong>sacct</strong> command:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>$ sacct

       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- -------- 
1633383          zipper       main      statx         16    RUNNING      0:0</code></pre></figure>
                    <p>Here, the state of each job is listed as being either PENDING, RUNNING, COMPLETED, or FAILED.</p>
                    <p>For complete and detailed job info, you can use the <strong>scontrol show job [JobID]</strong> command:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>$ scontrol show job 244348

JobId=244348 JobName=XIoT22
   UserId=gc563(148267) GroupId=gc563(148267) MCS_label=N/A
   Priority=5050 Nice=0 Account=oarc QOS=normal
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=1-04:07:40 TimeLimit=2-00:00:00 TimeMin=N/A
   SubmitTime=2017-05-14T07:47:19 EligibleTime=2017-05-14T07:47:19
   StartTime=2017-05-14T07:47:21 EndTime=2017-05-16T07:47:21 Deadline=N/A
   PreemptTime=None SuspendTime=None SecsPreSuspend=0
   Partition=main AllocNode:Sid=amarel1:22391
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=hal0053
   BatchHost=hal0053
   NumNodes=1 NumCPUs=28 NumTasks=28 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   TRES=cpu=28,mem=124000M,node=1
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=1 MinMemoryNode=124000M MinTmpDiskNode=0
   Features=(null) Gres=(null) Reservation=(null)
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/scratch/gc563/run.STMV.CPU.slurm
   WorkDir=/scratch/gc563
   StdErr=/scratch/gc563/slurm.%N.244348.out
   StdIn=/dev/null
   StdOut=/scratch/gc563/slurm.%N.244348.out
   Power=
</code></pre></figure>
                </div>



                <div class=bs-docs-section>
                    <h1 class=page-header id=killingjobs>Killing / cancelling / terminating jobs</h1>
                    <p>To terminate a job, regardless of whether it is running or just waiting in the job queue, use the <strong>scancel</strong> command and specify the JobID number of the job you wish to terminate:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>$ scancel 1633383</code></pre></figure>
                    <p>A job can only be cancelled by the owner of that job. When you terminate a job, a message from the SLURM workload manager will be directed to STDERR and that message will look like this:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>slurmstepd: *** JOB 1633383 ON slepner036 CANCELLED AT 2016-10-04T15:38:07 ***</code></pre></figure>
                </div>



                <div class=bs-docs-section>
                    <h1 class=page-header id=installingsoftware>Installing your own software</h1>
                    <p>Package management systems like yum or apt-get, which are used to install software in typical Linux systems, are not available to users of shared computing resources like Amarel. Thus, most packages need to be compiled from their source code and then installed. Further, most packages are generally configured to be installed in /usr or /opt, but these locations are inaccessible to (not writeable for) general users. Special care must be taken by users to ensure that the packages will be installed in their own /home directory (/home/[NetID]).</p>
                    <p>As an example, here are the steps for installing ZIPPER, a generic example package that doesn’t actually exist:</p>

                    <p>(1) Download your software package. You can usually download a software package to your laptop, and then transfer the downloaded package to your /home/[NetID] directory on Amarel for installation. Alternatively, if you have the http or ftp address for the package, you can transfer that package directly to your home directory while logged-in to Amarel using the <strong>wget</strong> utility:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>$ wget http://www.zippersimxl.org/public/zipper/zipper-4.1.5.tar.gz</code></pre></figure>
                    <p>(2) Unzip and unpack the .tar.gz (or .tgz) file. Most software packages are compressed in a .zip, .tar or .tar.gz file. You can use the <strong>tar</strong> utility to unpack the contents of these files:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>$ tar -zxf zipper-4.1.5.tar.gz</code></pre></figure>
                    <p>(3) Read the instructions for installing. Several packages come with an INSTALL or README script with instructions for setting up that package. Many will also explicitly include instructions on how to do so on a system where you do not have root access. Alternatively, the installation instructions may be posted on the website from which you downloaded the software.</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>$ cd zipper-4.1.5
$ less README</code></pre></figure>
                    <p>(4) Load the required software modules for installation. Software packages generally have dependencies, i.e., they require other software packages in order to be installed. The README or INSTALL file will generally list these dependencies. Often, you can use the available modules to satisfy these dependencies. But sometimes, you may also need to install the dependencies for yourself. Here, we load the dependencies for ZIPPER:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>$ module load intel/16.0.3 mvapich2/2.1</code></pre></figure>
                    <p>(5) Perform the installation. The next few steps vary widely but instructions almost always come with the downloaded source package. Guidance on the special arguments passed to the configure script is often available by running the <strong>./configure -–help</strong> command. What you see below is just a typical example of special options that might be specified.</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>$ ./configure --prefix=/home/[NetID]/zipper/4.1.5 --disable-float --enable-mpi --without-x --disable-shared
$ make -j 4
$ make install</code></pre></figure>
                    <p>Several packages are set up in a similar way, i.e., using configure, then make, and make install. Note the options provided to the configure script – these differ from package to package, and are documented as part of the setup instructions, but the prefix option is almost always supported. It specifies where the package will be installed. Unless this special argument is provided, the package will generally be installed to a location such as /usr/local or /opt, but users do not have write-access to those directories. So, here, I'm installing software in my /home/[NetID]/zipper/4.1.5 directory. The following directories are created after installation:</p>

                    <p>/home/[NetID]/zipper/4.1.5/<strong>bin</strong> where executables will be placed</p>
                    <p>/home/[NetID]/zipper/4.1.5/<strong>lib</strong> where library files will be placed</p>
                    <p>/home/[NetID]/zipper/4.1.5/<strong>include</strong> where header files will be placed</p>
                    <p>/home/[NetID]/zipper/4.1.5/<strong>share/man</strong> where documentation will be placed</p>

                    <p>(6) Configure environment settings. The above bin, lib, include and share directories are generally not part of the shell environment, i.e., the shell and other programs don’t “know” about these directories. Therefore, the last step in the installation process is to add these directories to the shell environment:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>export PATH=/home/[NetID]/zipper/4.1.5/bin:$PATH
export C_INCLUDE_PATH=/home/[NetID]/zipper/4.1.5/include:$C_INCLUDE_PATH
export CPLUS_INCLUDE_PATH=/home/[NetID]/zipper/4.1.5/include:$CPLUS_INCLUDE_PATH
export LIBRARY_PATH=/home/[NetID]/zipper/4.1.5/lib:$LIBRARY_PATH
export LD_LIBRARY_PATH=/home/[NetID]/zipper/4.1.5/lib:$LD_LIBRARY_PATH
export MANPATH=/home/[NetID]/zipper/4.1.5/share/man:$MANPATH</code></pre></figure>
                    <p>These <strong>export</strong> commands are standalone commands that change the shell environment, but these new settings are only valid for the current shell session. Rather than executing these commands for every shell session, they can be added to the end of your ~/.bashrc file which will result in those commands being executed every time you log-in to Amarel.</p>
                </div>




                <div class=bs-docs-section>
                    <h1 class=page-header id=singularity>Singularity</h1>
                    <p><a href=http://singularity.lbl.gov>Singularity</a> is a Linux containerization tool suitable for HPC environments. It uses its own container format and also has features that enable importing Docker containers.</p>
                    <p><a href=https://www.docker.com>Docker</a> is a platform that employs features of the Linux kernel to run software in a container. The software housed in a Docker container is not standalone program but an entire OS distribution, or at least enough of the OS to enable the program to work. Docker can be thought of as somewhat like a software distribution mechanism like yum or apt. It also can be thought of as an expanded version of a chroot jail, or a reduced version of a virtual machine.</p>

					<h3>Important differences between Docker and Singularity:</h3>

				    <p><ul><li>Docker and Singularity have their own container formats.</li>
				    <li>Docker containers can be imported and run using Singularity.</li>
				    <li>Docker containers usually run as root, which means you cannot run Docker on a shared computing system (cluster).</li>
				    <li>Singularity allows for containers that can be run as a regular user. How? When importing a Docker container, Singularity removes any elements which can only run as root. The resulting containers can be run using a regular user account.</li></ul></p>

					<h3>Importing a Docker image:</h3>

					<p>If you have a pre-built Docker container, you can use Singularity to convert this container to the Singularity format. Once that's done, you can upload your Singularity container to your storage space on Amarel and run jobs using that container.</p>

					<p>Here's an example. NOTE that most of these steps are performed on your local system, not while logged-in on Amarel.</p>

					<p>If you need to use any of Amarel's filesystems inside your container, you will need to make sure the appropriate directories exist in your container so those filesystems can be mounted using those directories.</p>

					<p>Start your container (in this example we will use ubuntu:latest) and create directories for mounting /scratch/gc563 and /projects/oarc. Of course, you'll need to use directories that you can access on Amarel.</p>

					<figure class=highlight><pre><code class=language-html data-lang=html>$ sudo docker run -it ubuntu:latest bash
root@11a87dkw8748:/# mkdir -p /scratch/gc563 /projects/oarc</code></pre></figure>

					<h3>Exporting your Docker image</h3>

					<p>Find the name of your Docker image using the 'docker ps' command,</p>

					<figure class=highlight><pre><code class=language-html data-lang=html>$ sudo docker ps
CONTAINER ID  IMAGE          COMMAND  CREATED        STATUS        NAMES
11a87dkw8748  ubuntu:latest  "bash"   2 minutes ago  Up 2 minutes  bendakaya_pakodi</code></pre></figure>

					<p>In this example the name of the images is bendakaya_pakodi. Export this image to a tarball,</p>

					<figure class=highlight><pre><code class=language-html data-lang=html>$ sudo docker export bendakaya_pakodi > ubuntu.tar</code></pre></figure>

					<h3>Converting to a Singularity image</h3>

					<p>You will need to have Singularity installed on your local workstation/laptop to prepare your image. The 'create' and 'import' operations of Singularity require root privileges, which you do not have on Amarel.</p>

					<p>Create an empty singularity image, and then import the exported docker image into it,</p>

					<figure class=highlight><pre><code class=language-html data-lang=html>$ sudo singularity create ubuntu.img
Creating a sparse image with a maximum size of 1024MiB...
Using given image size of 1024
Formatting image (/sbin/mkfs.ext3)
Done. Image can be found at: ubuntu.img
$ sudo singularity import ubuntu.img ubuntu.tar</code></pre></figure>


					<h3>Using Singularity containers inside a SLURM job</h3>

					<p><a href=#movingfiles>Transfer</a> your new Singularity image to Amarel. The following steps are performed while logged-in to Amarel.</p>

					<p>You can run any task/program inside the container by prefacing it with 'singularity exec [your image name]'

					<p>Here is a simple example job script that executes commands inside a container,</p>

<figure class=highlight><pre><code class=language-html data-lang=html>#SBATCH --partition=main             # Partition (job queue)
#SBATCH --job-name=sing2me           # Assign an short name to your job
#SBATCH --nodes=1                    # Number of nodes you require
#SBATCH --ntasks=1                   # Total # of tasks across all nodes
#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)
#SBATCH --mem=4000                   # Real memory (RAM) required (MB)
#SBATCH --time=00:30:00              # Total run time limit (HH:MM:SS)
#SBATCH --output=slurm.%N.%j.out     # STDOUT output file

module purge
module load singularity/.2.5.1

## Where am I running?
srun singularity exec ubuntu.img hostname

## What is the current time and date?
srun singularity exec ubuntu.img date</code></pre></figure>

					<p>If you created directories for any Amarel filesystems, you should find they are mounted inside your container,</p>

<figure class=highlight><pre><code class=language-html data-lang=html>$ mount | grep gpfs
/dev/scratch/gc563 on /scratch/gc563 type gpfs (rw,relatime)
/dev/projects/oarc on /projects/oarc type gpfs (rw,relatime)</code></pre></figure>

					<p> NOTE: If your container mounts Amarel directories, software inside the container may be able to destroy data on these filesystems for which you have write permissions. Proceed with caution.</p>

                </div>




<div class=bs-docs-section>
                    <h1 class=page-header id=examples>Examples</h1>

<h2>Example: Using R</h2>
<p>Generally, there are 2 approaches for accessing R on Amarel: (1) use one of the pre-installed R modules named R-Project/[version] (these versions come bundled with a very broad range of common and useful tools), or (2) install your own custom build of R in your /home directory or in a shared directory (e.g., /projects/[group] or /projects/community).</p>

<p class=lead>Using pre-installed R modules:</p>

<p>Start by finding which module you wish to use with the 'module spider R-Project' command:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>$ module spider R-Project
--------------------------------------------------
  R-Project:
--------------------------------------------------
    Description:
      R: The R Project for Statistical Computing
     Versions:
        R-Project/3.2.2
        R-Project/3.2.5
        R-Project/3.3.3
        R-Project/3.4.1
--------------------------------------------------
  To find detailed information about R-Project please enter the full name.
  For example:
     $ module spider R-Project/3.4.1
--------------------------------------------------</code></pre></figure>

<p>Next, use 'module spider' again to see how to load the module you wish to use (e.g., are there any prerequisites that must be loaded first?):</p>
<figure class=highlight><pre><code class=language-html data-lang=html>$ module spider R-Project/3.4.1
--------------------------------------------------
  R-Project: R-Project/3.4.1
--------------------------------------------------
    Description:
      R: The R Project for Statistical Computing
    This module can only be loaded through the following modules:
      intel/17.0.4
    Help:    
      This module loads the installation R-Project 3.4.1 compiled with the Intel 17.0.4 compilers.</code></pre></figure>

<p>Load the R-Project module of your choice:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>$ module load intel/17.0.4 R-Project/3.4.1
$ which R
/opt/sw/packages/intel-17.0.4/R-Project/3.4.1/bin/R</code></pre></figure>


<p>What R packages are already installed?</p>
<figure class=highlight><pre><code class=language-html data-lang=html>> pkgs <- installed.packages ()
> pkgs[,c("Package", "Version")]

                     Package                Version    
base                 "base"                 "3.4.4"    
BH                   "BH"                   "1.66.0-1" 
Biobase              "Biobase"              "2.38.0"   
BiocGenerics         "BiocGenerics"         "0.24.0"   
BiocInstaller        "BiocInstaller"        "1.28.0"   
BiocParallel         "BiocParallel"         "1.12.0"   
Biostrings           "Biostrings"           "2.46.0"   
bitops               "bitops"               "1.0-6"    
boot                 "boot"                 "1.3-20"   
BSgenome             "BSgenome"             "1.46.0"
...</code></pre></figure>


<p>It's very common for uers to need additional or custom packages for a base R installation. On a large, shared computing system, users are unable to install (write) to the usual places where R places new packages by default (/usr/local or /usr/lib). Therefore, managing your own local package/library installation location is necessary. In the example below, I'll demonstrate how I did this for my Amarel user account.</p>

<p>First, I'll create a directory where I can store my locally-installed R packages. This can have any name and it can be located anywhere you have access:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>mkdir ~/my.R.libs</code></pre></figure>
<p>Next, to ensure that my new private R packages directory is searched when I try to load a library that's installed there, I need to make an environment setting that will point R to the right location. I'll create a new file in my /home directory named .Renviron (note the leading "." in that name) and I'll add the following line to that file:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>export R_LIBS=~/my.R.libs</code></pre></figure>
<p>Now, every time I start any version of R, my ~/my.R.libs directory will be the first location to be searched when loading a library.</p> 

<br>
<p><strong>Some important notes about installing packages:</strong></p>

<p>There are a variety of different ways to install packages in R. The most straightforward way is to use the built-in 'install.packages()' function while R is running. Using this approach gives you the flexibility to install the latest version of a package or you can specify an older version of a package. To install a specifc version of a package, you'll need the URL (web address) for the tarball (*.tar.gz or *.tgz file) containing the source code for that version.</p>
<p>For example, I want to load the following list of packages, and I need the specifc versions listed here:
'kernlab' version 0.9-24
'ROCR' version 1.0.7
'class' version 7.3.14
'party' version 1.0.25
'e1071' version 1.6.7
'randomForest' version 4.6.12
I can use a web search to find the source tarballs for these packages. For example, to find 'kernlab' version 0.9-24, I search for "kernlab" and find the website, https://cran.r-project.org/web/packages/kernlab/index.html. At that site, I see that 0.9-25 is the current version (not what I want), but there is "kernlab archive" link there that takes me to a long list of previous versions. I see a link for version 0.9-24 there, so I copy that URL and use that URL in my install.packages() command:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>> install.packages("https://cran.r-project.org/src/contrib/Archive/kernlab/kernlab_0.9-24.tar.gz", lib="~/my.R.libs")</code></pre></figure>
<p>The other packages I need can be found in the same way. While installing them, I find that 'ROCR-1.0.7' requires 'gplots' and 'party-1.0-25' requires 6 other prerequisites. So, I have to also install those prerequisite packages. In the end, my install.packages() commands are as follows:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>> install.packages("gplots", lib="~/my.R.libs")
> install.packages("https://cran.r-project.org/src/contrib/ROCR_1.0-7.tar.gz", lib="~/my.R.libs")
> install.packages("https://cran.r-project.org/src/contrib/class_7.3-14.tar.gz", lib="~/my.R.libs")
> install.packages(c("mvtnorm","modeltools","strucchange","coin","zoo","sandwich"), lib="~/my.R.libs")
> install.packages("https://cran.r-project.org/src/contrib/Archive/party/party_1.0-25.tar.gz", lib="~/my.R.libs")
> install.packages("https://cran.r-project.org/src/contrib/Archive/e1071/e1071_1.6-7.tar.gz", lib="~/my.R.libs")
> install.packages("https://cran.r-project.org/src/contrib/Archive/randomForest/randomForest_4.6-12.tar.gz", lib="~/my.R.libs")</code></pre></figure>
<p>Once all of my package installs have completed successfully, those packages can be loaded normally and they will be available every time I log-in to the cluster.</p>


<br>
<p class=lead>Installing your own build of R:</p>

<p>For some users or groups, installing and customizing or even modifying the latest version (or a specific version) of R is necessary. For those users, I'll demonstrate how to install a version of R below.</p>

<p>Here are the commands to use for installing R-3.4.4 from source:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>wget https://cran.r-project.org/src/base/R-3/R-3.4.4.tar.gz
tar -zxf R-3.4.4.tar.gz
cd R-3.4.4
module load gcc/5.4 java/1.8.0_161
./configure --prefix=/home/gc563/R/3.4.4 --enable-java
make -j 4
make install
cd ..
rm -rf R-3.4.4*</code></pre></figure>

<p>Here, I have loaded the GCC compiler suite and Java before installing R. This is an optional step and I did it because there might be specific tools I will use with R that require these extra software packages.</p>
<p>When I configured my installation, I specified (with --prefix=) that I want R to be installed in my /home directory. I prefer to use a [package]/[version] structure because that enables easy organization of multiple verisons of any software package. It's a personal preference.</p>
<p>At the end of my install procedure, I remove the downloaded install package and tarball, just to tidy-up.</p>

<br>
<p>Since I've installed R in my /home directory, I can add packages using the default library directory since that too will be in my /home directory.</p>

<br>
<p>Before using my new R installation, I'll need to set some environment variables and load needed modules (the same ones I used for building my R installation). This can be done from the command line (but the settings won't persist after you log-out) or by adding these commands to the bottom of your ~/.bashrc file (so the settings will persist):</p>
<figure class=highlight><pre><code class=language-html data-lang=html>module load gcc/5.4 java/1.8.0_161
export PATH=/home/gc563/R/3.4.4/bin:$PATH
export LIBRARY_PATH=/home/gc563/R/3.4.4/lib64
export LD_LIBRARY_PATH=/home/gc563/R/3.4.4/lib64
export MANPATH=/home/gc563/R/3.4.4/share/man</code></pre></figure>
<p>If you're adding these lines to the bottom of your ~/.bashrc file, log-out and log-in again, then verify that the settings are working:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>$ module list
Currently Loaded Modules:
  1) gcc/5.4   2) java/1.8.0_161
$ which R
~/R/3.4.4/bin/R</code></pre></figure>

<p>Now that my new R installation is setup, I can begin adding R packages. Since this is my own installation of R and not one of the preinstalled versions available on the cluster, my default packages/libraries directory is /home/gc563/R/3.4.4/lib64/R/library</p>
<figure class=highlight><pre><code class=language-html data-lang=html>> .libPaths()
[1] "/home/gc563/R/3.4.4/lib64/R/library"</code></pre></figure>
<p>Install a package:</p>

<figure class=highlight><pre><code class=language-html data-lang=html>> install.packages("rJava")
> library(rJava)
> </code></pre></figure>


<br>
<p class=lead>Saving figures/plots from R (without a display):</p>
<p>Need to save a PDF, PostScript, SVG, PNG, JPG, or TIFF file in your working directory? Normally, writing a graphics file from R requires a display of some kind and the X11 protocol. That's often not convenient for batch jobs running on the cluster. Alternatively, you can use the Cairo graphics device/library for R. Cairo enables you to write bitmap or vector graphics directly to a file. Here's an example:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>$ R --no-save
> png('my-figure.png', type='cairo')
> plot(rnorm(10),rnorm(10))
> dev.off()
> q()</code></pre></figure>







<br>
<h2>Example: Using Python</h2>
<p>Generally, there are 2 approaches for using Python and its associated tools: (1) use one of the pre-installed Python modules (version 2.7.x or 3.5.x) which come bundled with a very broad range of common and useful tools (you can add or update packages if needed) or (2) install your own custom build of Python in your /home directory or in a shared directory (e.g., /projects/[group] or /projects/community).</p>

<p class=lead>Using pre-installed Python modules:</p>

<p>With the pre-installed Python modules, you can add or update Python modules/packages as needed if you do it using the '--user' option for pip. This option will instruct pip to install new software or upgrades in your ~/.local directory. Here's an example where I'm installing the Django package:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>$ module load python/3.5.2
$ pip install --user Django</code></pre></figure>

<p>Note: if necessary, pip can also be upgraded when using a system-installed build of Python, but be aware that the upgraded version of pip will be installed in ~/.local/bin. Whenever a system-installed Pytyon module is loaded, the PATH location of that module's executables (like pip) will precede your ~/.local/bin directory. To run the upgraded version of pip, you'll need to specify its location because the previous version of pip will no longer work properly:
<figure class=highlight><pre><code class=language-html data-lang=html>$ which pip
/opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip
$ pip --version
pip 9.0.3 from /opt/sw/packages/gcc-4.8/python/3.5.2/lib/python3.5/site-packages (python 3.5)
$ pip install -U --user pip
Successfully installed pip-10.0.1
$ which pip
/opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip
$ pip --version
Traceback (most recent call last):
  File "/opt/sw/packages/gcc-4.8/python/3.5.2/bin/pip", line 7, in <module>
    from pip import main
ImportError: cannot import name 'main'

$ .local/bin/pip --version
pip 10.0.1 from /home/gc563/.local/lib/python3.5/site-packages/pip (python 3.5)
$ .local/bin/pip install --user Django
</code></pre></figure>

<br>
<p class=lead>Building your own Python installation:</p>

<p>Using this approach, I must specify that I want Python to be installed in my /home directory. This is done using the '--prefix=' option. Also, I prefer to use a [package]/[version] naming scheme because that enables easy organization of multiple verisons of Python (optional, it's just a personal preference).</p>
<p>At the end of my install procedure, I remove the downloaded install package and tarball, just to tidy-up.</p>
<figure class=highlight><pre><code class=language-html data-lang=html>wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz
tar -zxf Python-3.6.5.tgz
cd Python-3.6.5
./configure --prefix=/home/gc563/python/3.6.5
make -j 4
make install
cd ..
rm -rf Python-3.6.5*</code></pre></figure>

<p>Before using my new Python installation, I'll need to set or edit some environment variables. This can be done from the command line (but the settings won't persist after you log-out) or by adding these commands to the bottom of your ~/.bashrc file (so the settings will persist):</p>
<figure class=highlight><pre><code class=language-html data-lang=html>export PATH=/home/gc563/python/3.6.5/bin:$PATH
export LD_LIBRARY_PATH=/home/gc563/python/3.6.5/lib
export MANPATH=/home/gc563/python/3.6.5/share/man</code></pre></figure>
<p>If you're adding these lines to the bottom of your ~/.bashrc file, log-out and log-in again, then verify that the settings are working:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>$ which python3
~/python/3.6.5/bin</code></pre></figure>






<br>
                    <h2 class=page-header>Example: Running GROMACS</h2>
                    <p>Here is a simple example procedure that demonstrates how to use GROMACS 2016 on Amarel. In this example, we’ll start with a downloaded PDB file and proceed through importing that file into GROMACS, solvating the protein, a quick energy minimization, and then an MD equilibration. This example is not intended to teach anyone how to use GROMACS. Instead, it is intended to assist new GROMACS users in learning to use GROMACS on Amarel.</p>
                    <p>(1) Download a PDB file.</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>wget https://files.rcsb.org/view/5EWT.pdb</code></pre></figure>
                    <p>(2) Load the GROMACS software module plus any needed prerequisites.</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>module purge
module load intel/17.0.1 mvapich2/2.2 gromacs/2016.1</code></pre></figure>
                    <p>(3) Import the PDB into GROMACS, while defining the force field and water model to be used for this system.</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>gmx_mpi pdb2gmx -f 5EWT.pdb -ff charmm27 -water tip3p -ignh -o 5EWT.gro -p 5EWT.top -i 5EWT.itp</code></pre></figure>
                    <p>(4) Increase the size of the unit cell to accommodate a reasonable volume of solvent around the protein.</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>gmx_mpi editconf -f 5EWT.gro -o 5EWT_newbox.gro -box 10 10 10 -center 5 5 5</code></pre></figure>
                    <p>(5) Now add water molecules into the empty space in the unit cell to solvate the protein.</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>gmx_mpi solvate -cp 5EWT_newbox.gro -p 5EWT.top -o 5EWT_solv.gro</code></pre></figure>
                    <p>(6) Prepare your SLURM job script(s). The 2 <strong>mdrun</strong> commands in the following steps can be executed from within an interactive session or they can be run in batch mode using job scripts. If your <strong>mdrun</strong> commands/job might take more than a few minutes to run, it would be best to run them in batch mode using a job script. Here’s an example job script for a GROMACS MD simulation. To run the 2 <strong>mdrun</strong> commands below, simply replace the example <strong>mdrun</strong> command in this script with one of the <strong>mdrun</strong> commands from the steps below and submit that job after preparing the simulation with the appropriate <strong>grompp</strong> step.</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>#!/bin/bash
#SBATCH --partition=main                # Partition (job queue)
#SBATCH --job-name=gmdrun               # Assign an 8-character name to your job
#SBATCH --nodes=1                       # Number of nodes
#SBATCH --ntasks=16                     # Total # of tasks across all nodes
#SBATCH --cpus-per-task=1               # Threads per process (or per core)
#SBATCH --mem=124000                    # Memory per node (MB)
#SBATCH --time=00:20:00                 # Total run time limit (HH:MM:SS)
#SBATCH --output=slurm.%N.%j.out        # combined STDOUT and STDERR output file
#SBATCH --export=ALL                    # Export you current env to the job env

module purge
module load intel/17.0.1 mvapich2/2.2 gromacs/2016.1
srun --mpi=pmi2 gmx_mpi mdrun -v -s 5EWT_solv_prod.tpr \
                -o 5EWT_solv_prod.trr -c 5EWT_solv_prod.gro \
                -e 5EWT_solv_prod.edr -g 5EWT_solv_prod.md.log</code></pre></figure>
                    <p>(7) Perform an inital, quick energy minimization. Here, we’re using a customized MD parameters file named em.mdp, which contains these instructions:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>integrator     = steep
nsteps         = 200
cutoff-scheme  = Verlet
coulombtype    = PME
pbc            = xyz
emtol          = 100</code></pre></figure>
                    <p>These are the commands (both the <strong>grompp</strong> step and the <strong>mdrun</strong> step) used to prepare and run the minimization:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>gmx_mpi grompp -f em.mdp -c 5EWT_solv.gro -p 5EWT.top -o 5EWT_solv_mini.tpr -po 5EWT_solv_mini.mdp

gmx_mpi mdrun -v -s 5EWT_solv_mini.tpr -o 5EWT_solv_mini.trr -c 5EWT_solv_mini.gro -e 5EWT_solv_mini.edr -g 5EWT_solv_mini.md.log</code></pre></figure>
                    <p>(8) Perform a quick MD equilibration (same syntax/commands for a regular MD run). Here, we’re using a customized MD parameters file named equil.mdp, which contains these instructions:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>integrator               = md
dt                       = 0.002
nsteps                   = 5000
nstlog                   = 50
nstenergy                = 50
nstxout                  = 50
continuation             = yes
constraints              = all-bonds
constraint-algorithm     = lincs
cutoff-scheme            = Verlet
coulombtype              = PME
rcoulomb                 = 1.0
vdwtype                  = Cut-off
rvdw                     = 1.0
DispCorr                 = EnerPres
tcoupl                   = V-rescale
tc-grps                  = Protein  SOL
tau-t                    = 0.1      0.1
ref-t                    = 300      300
pcoupl                   = Parrinello-Rahman
tau-p                    = 2.0
compressibility          = 4.5e-5
ref-p                    = 1.0</code></pre></figure>
                    <p>These are the commands (both the <strong>grompp</strong> step and the mdrun step) used to prepare and run the equilibration:</p>
                    <figure class=highlight><pre><code class=language-html data-lang=html>gmx_mpi grompp -f equil.mdp -c 5EWT_solv_mini.gro -p 5EWT.top -o 5EWT_solv_equil.tpr -po 5EWT_solv_equil.mdp

gmx_mpi mdrun -v -s 5EWT_solv_equil.tpr -o 5EWT_solv_equil.trr -c 5EWT_solv_equil.gro -e 5EWT_solv_equil.edr -g 5EWT_solv_equil.md.log</code></pre></figure>





<br>
<h2 class=page-header >Example: Running TensorFlow with a GPU</h2>
<p>To do this, you can use the <a href=http://singularity.lbl.gov>Singularity</a> container manager and a Docker image containing the TensorFlow software.</p>
<p>Running Singularity can be done in batch mode using a job script. Below is an example job script for this purpose (for this example, we'll name this script TF_gpu.sh)</p>
 
<figure class=highlight><pre><code class=language-html data-lang=html>
#!/bin/bash
#SBATCH --partition=main             # Partition (job queue)
#SBATCH --no-requeue                 # Do not re-run job  if preempted
#SBATCH --job-name=TF_gpu            # Assign an short name to your job
#SBATCH --nodes=1                    # Number of nodes you require
#SBATCH --ntasks=1                   # Total # of tasks across all nodes
#SBATCH --cpus-per-task=2            # Cores per task (>1 if multithread tasks)
#SBATCH --gres=gpu:1                 # Number of GPUs
#SBATCH --mem=16000                  # Real memory (RAM) required (MB)
#SBATCH --time=00:30:00              # Total run time limit (HH:MM:SS)
#SBATCH --output=slurm.%N.%j.out     # STDOUT output file
#SBATCH --error=slurm.%N.%j.err      # STDERR output file (optional)
#SBATCH --export=ALL                 # Export you current env to the job env

module purge
module load singularity/.2.5.1

srun singularity exec --nv docker://tensorflow/tensorflow:1.4.1-gpu python <your *.py file>
</code></pre></figure>

<p>Once your job script is ready, submit it using the sbatch command:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>
$ sbatch TF_gpu.sh</code></pre></figure>

<p>Alternatively, you can run Singularity interactively:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>
$ srun --pty -p main --gres=gpu:1 --time=15:00 --mem=6G singularity shell --nv docker://tensorflow/tensorflow:1.4.1-gpu

Docker image path: index.docker.io/tensorflow/tensorflow:1.4.1-gpu
Cache folder set to /home/user/.singularity/docker
Creating container runtime...
Importing: /home/user/.singularity/docker/sha256:054be6183d067af1af06196d7123f7dd0b67f7157a0959bd857ad73046c3be9a.tar.gz
Importing: /home/user/.singularity/docker/sha256:779578d7ea6e8cc3934791724d28c56bbfc8b1a99e26236e7bf53350ed839b98.tar.gz
Importing: /home/user/.singularity/docker/sha256:82315138c8bd2f784643520005a8974552aaeaaf9ce365faea4e50554cf1bb44.tar.gz
Importing: /home/user/.singularity/docker/sha256:88dc0000f5c4a5feee72bae2c1998412a4b06a36099da354f4f97bdc8f48d0ed.tar.gz
Importing: /home/user/.singularity/docker/sha256:79f59e52a355a539af4e15ec0241dffaee6400ce5de828b372d06c625285fd77.tar.gz
Importing: /home/user/.singularity/docker/sha256:ecc723991ca554289282618d4e422a29fa96bd2c57d8d9ef16508a549f108316.tar.gz
Importing: /home/user/.singularity/docker/sha256:d0e0931cb377863a3dbadd0328a1f637387057321adecce2c47c2d54affc30f2.tar.gz
Importing: /home/user/.singularity/docker/sha256:f7899094c6d8f09b5ac7735b109d7538f5214f1f98d7ded5756ee1cff6aa23dd.tar.gz
Importing: /home/user/.singularity/docker/sha256:ecba77e23ded968b9b2bed496185bfa29f46c6d85b5ea68e23a54a505acb81a3.tar.gz
Importing: /home/user/.singularity/docker/sha256:037240df6b3d47778a353e74703c6ecddbcca4d4d7198eda77f2024f97fc8c3d.tar.gz
Importing: /home/user/.singularity/docker/sha256:b1330cb3fb4a5fe93317aa70df2d6b98ac3ec1d143d20030c32f56fc49b013a8.tar.gz
Importing: /home/user/.singularity/metadata/sha256:b71a53c1f358230f98f25b41ec62ad5c4ba0b9d986bbb4fb15211f24c386780f.tar.gz
Singularity: Invoking an interactive shell within container...

Singularity tensorflow:latest-gpu:~> 
</code></pre></figure>

<p>Now, you're ready to execute commands:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>
Singularity tensorflow:latest-gpu:~> python -V
Python 2.7.12
Singularity tensorflow:latest-gpu:~> python3 -V
Python 3.5.2
</code></pre></figure>

<p>Please remember to exit from your interactive job after you are finished with your calculations.</p>

<p>There are several Docker images available on Amarel for use with Singularity. The one used in the example above, tensorflow1.4.1-gpu, is intended for python 2.7.12. If you want to use Python3, you'll need a different image, docker://tensorflow/tensorflow:1.4.1-gpu-py3, and the Python command will be 'python3' instead of 'python' in your script.</p>  
</div>

                <div class=bs-docs-section>
                    <h1 class=page-header id=troubleshooting>Troubleshooting / Common Problems</h1>
                    <p class=lead>Failure to load module dependencies/prerequisites:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>                    
$ module load R-Project/3.4.1
Lmod has detected the following error:  These module(s) exist but cannot be loaded as requested: "R-Project/3.4.1"
Try: "module spider R-Project/3.4.1" to see how to load the module(s).</code></pre></figure>
<p>This software module has a prerequisite module that must be loaded first. To find out what prerequisite module is required, use the 'module spider' command followed by the name of the module you're trying to load:</p>
<figure class=highlight><pre><code class=language-html data-lang=html>                    
$ module spider R-Project/3.4.1
    This module can only be loaded through the following modules:
      intel/17.0.4
    Help: 
      This module loads the installation R-Project 3.4.1 compiled with the Intel 17.0.4 compilers.
</code></pre></figure>
<p>Ah-ha, it looks like the intel/17.0.4 module must be loaded before loading R-Project/3.4.1</p>
                </div>


                <div class=bs-docs-section>
                    <h1 class=page-header id=citing>Acknowledging Amarel</h1>
                    <p>Please reference OARC and the Amarel cluster in any research report, journal or publication that requires citation of an author's work. Recognizing the OARC resources you used to conduct your research is important for our process of acquiring funding for hardware, support services, and other infrastructure improvements. The minimal content of a reference should include:</p>

<p><i>Office of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey</i></p>

<p>A suggested acknowledgement is:</p>

<p><i>The authors acknowledge the Office of Advanced Research Computing (OARC) at Rutgers, The State University of New Jersey for providing access to the Amarel cluster and associated research computing resources that have contributed to the results reported here. URL: http://oarc.rutgers.edu</i></p>
                </div>


            </div>
            <div class=col-md-3 role=complementary>
                <nav class="bs-docs-sidebar hidden-print hidden-sm hidden-xs">
                    <ul class="nav bs-docs-sidenav">
                        <li> <a href=#generalinfo>General info</a> </li>
                        <li> <a href=#connecting>Connecting to Amarel</a></li>
                        <li> <a href=#movingfiles>Moving files</a></li>
                        <li> <a href=#listingresources>Listing resources</a> </li>
                        <li> <a href=#softwaremodules>Software modules</a></li>
                        <li> <a href=#serialjobs>Serial jobs</a></li>
                        <li> <a href=#paralleljobs>Parallel jobs</a> </li>
                        <li> <a href=#interactivejobs>Interactive jobs</a> </li>
                        <li> <a href=#monitoringjobs>Monitoring jobs</a> </li>
                        <li> <a href=#killingjobs>Killing jobs</a></li>
                        <li> <a href=#installingsoftware>Installing software</a> </li>
                        <li> <a href=#singularity>Using Singularity</a> </li>
                        <li> <a href=#examples>Examples</a> </li>
                        <li> <a href=#troubleshooting>Troubleshooting</a> </li>
                        <li> <a href=#citing>Acknowledging Amarel</a> </li>
                    </ul> <a href=#top class=back-to-top> Back to top </a> </nav>
            </div>
        </div>
    </div>
    <footer class=bs-docs-footer>
        <div class=container>
            <ul class=bs-docs-footer-links>
                <li><a href=http://oarc.rutgers.edu>Events</a></li>
                <li><a href=http://oarc.rutgers.edu>Research</a></li>
                <li><a href=http://oarc.rutgers.edu>People</a></li>
            </ul>
            <p>Comments or questions should be directed to <strong>Galen Collier</strong> ( <a href=mailto:galen.collier@rutgers.edu>galen.collier@rutgers.edu</a>, Rutgers office: 848.445.5237 )</p>
            <br>
            <p><a href=http://oarc.rutgers.edu>OARC</a> 2018</p>
        </div>
    </footer>
    <script src=https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js></script>
    <script src=bootstrap.min.js></script>
    <script src=docs.min.js></script>
    <script>
        var _gauges = _gauges || [];
        ! function() {
            var e = document.createElement("script");
            e.async = !0, e.id = "gauges-tracker", e.setAttribute("data-site-id", "4f0dc9fef5a1f55508000013"), e.src = "//secure.gaug.es/track.js";
            var t = document.getElementsByTagName("script")[0];
            t.parentNode.insertBefore(e, t)
        }()
    </script>